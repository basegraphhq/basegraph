{
  "version": "1.0",
  "issue_context": "Token Consumption - Track token spent per issue",
  "fixtures": [
    {
      "id": "token-tracking-in-agents",
      "intent": "Understanding how tokens are currently tracked in planner and explore agents",
      "keywords": ["token", "tracking", "planner", "explore", "prompt_tokens", "completion_tokens", "accumulation", "usage"],
      "scope": ["brain/planner.go", "brain/explore_agent.go"],
      "report": "## Answer\nToken usage (prompt/completion) is accumulated in both Planner and ExploreAgent during their execution loops, but only logged - not persisted to database.\n\n## Evidence\n- brain/planner.go:113-125 — Planner accumulates `totalPromptTokens += resp.PromptTokens` and `totalCompletionTokens += resp.CompletionTokens` per iteration\n- brain/explore_agent.go:126-138 — ExploreAgent does the same accumulation pattern\n- brain/explore_agent.go:178-189 — Synthesis call also adds to totals\n\n## Snippets\n```go\n// planner.go:113-125\ntotalPromptTokens += resp.PromptTokens\ntotalCompletionTokens += resp.CompletionTokens\nslog.DebugContext(ctx, \"planner iteration completed\", \"prompt_tokens\", resp.PromptTokens)\n```\n\n## Confidence\nhigh — Direct code inspection confirms the pattern\n\n---\n\n**Confidence Assessment:** high — The token tracking logic is explicit and consistent across both agents."
    },
    {
      "id": "llm-eval-persistence",
      "intent": "Understanding LLMEval model and how it could persist token usage",
      "keywords": ["LLMEval", "persistence", "database", "store", "issue_id", "prompt_tokens", "completion_tokens", "save", "persist"],
      "scope": ["model/llm_eval.go", "store/llm_eval.go", "store/interfaces.go"],
      "report": "## Answer\nThere is an existing `LLMEval` model with `IssueID`, `PromptTokens`, and `CompletionTokens` fields, plus a store with `Create` and `ListByIssue` methods. However, no code currently calls the store to persist token usage.\n\n## Evidence\n- model/llm_eval.go:14-31 — `LLMEval` struct has `IssueID *int64`, `PromptTokens *int`, `CompletionTokens *int`\n- store/llm_eval.go:20-48 — `InsertLLMEval` persists these fields\n- store/interfaces.go:144-152 — `LLMEvalStore` interface supports `Create`, `ListByIssue`, `GetStats`\n\n## Snippets\n```go\n// model/llm_eval.go\ntype LLMEval struct {\n    ID               int64   `json:\"id\"`\n    IssueID          *int64  `json:\"issue_id,omitempty\"`\n    Stage            string  `json:\"stage\"`\n    PromptTokens     *int    `json:\"prompt_tokens,omitempty\"`\n    CompletionTokens *int    `json:\"completion_tokens,omitempty\"`\n}\n```\n\n## Confidence\nhigh — Model and store exist but are unused\n\n---\n\n**Confidence Assessment:** high — The infrastructure exists, just needs to be wired up."
    },
    {
      "id": "issue-model-structure",
      "intent": "Understanding Issue model and what fields it has",
      "keywords": ["Issue", "model", "struct", "fields", "CodeFindings", "Keywords", "Discussions", "schema"],
      "scope": ["model/issue.go", "store/issue.go"],
      "report": "## Answer\nThe `Issue` model stores issue metadata, discussions, keywords, code findings, and processing state. It does NOT currently have token usage fields.\n\n## Evidence\n- model/issue.go:83-109 — Issue struct with ID, IntegrationID, Title, Description, Labels, Keywords, CodeFindings, Discussions, Spec, ProcessingStatus\n- store/issue.go:21-61 — UpsertIssueParams marshals Keywords/CodeFindings/Learnings/Discussions as JSON\n\n## Snippets\n```go\n// model/issue.go:83-109\ntype Issue struct {\n    ID                int64         `json:\"id\"`\n    IntegrationID     int64         `json:\"integration_id\"`\n    Keywords          []Keyword     `json:\"keywords,omitempty\"`\n    CodeFindings      []CodeFinding `json:\"code_findings,omitempty\"`\n    Discussions       []Discussion  `json:\"discussions,omitempty\"`\n    ProcessingStatus  ProcessingStatus `json:\"processing_status\"`\n}\n```\n\n## Confidence\nhigh — Comprehensive view of Issue model\n\n---\n\n**Confidence Assessment:** high — Direct struct inspection."
    },
    {
      "id": "orchestrator-engagement-flow",
      "intent": "Understanding how orchestrator processes issues and where token tracking could be added",
      "keywords": ["orchestrator", "HandleEngagement", "IssueID", "EventLogID", "planner", "context", "flow", "processing"],
      "scope": ["brain/orchestrator.go"],
      "report": "## Answer\nOrchestrator's `HandleEngagement` loads an issue, runs the planner, and executes actions. IssueID is available in context throughout. This is the ideal place to persist aggregated token counts after planning completes.\n\n## Evidence\n- brain/orchestrator.go:19-24 — `EngagementInput` has `IssueID int64`, `EventLogID int64`\n- brain/orchestrator.go:94-100 — Context enriched with IssueID via `logger.WithLogFields`\n- brain/orchestrator.go:134-136 — `output, err := o.planner.Plan(ctx, messages)` is where planning happens\n- brain/orchestrator.go:94-165 — After planning, actions are executed, then function returns (end-of-processing hook point)\n\n## Snippets\n```go\n// brain/orchestrator.go:94-100\nctx = logger.WithLogFields(ctx, logger.LogFields{\n    IssueID:    &input.IssueID,\n    EventLogID: &input.EventLogID,\n})\n```\n\n## Confidence\nhigh — Clear insertion point identified\n\n---\n\n**Confidence Assessment:** high — The orchestrator flow is well-defined and IssueID is available."
    },
    {
      "id": "http-api-routes",
      "intent": "Understanding what HTTP API routes exist and whether there's an issue endpoint",
      "keywords": ["HTTP", "API", "routes", "router", "issue", "endpoint", "v1", "REST"],
      "scope": ["http/router/router.go", "http/dto"],
      "report": "## Answer\nThe HTTP API currently has `/api/v1/users`, `/api/v1/organizations`, and `/api/v1/integrations/gitlab` routes. There is NO `/api/v1/issues` endpoint yet.\n\n## Evidence\n- http/router/router.go:25-35 — Only registers users/orgs/gitlab routes under `/api/v1`\n- http/dto/*.go — DTO patterns exist for user/org but no IssueResponse DTO\n- http/middleware/auth.go:22-24 — TODO comment mentions auth middleware not applied to v1 routes yet\n\n## Snippets\n```go\n// http/router/router.go:17-35\nv1 := router.Group(\"/api/v1\") {\n    // users, organizations, gitlab...\n    // NO issues route\n}\n```\n\n## Confidence\nhigh — No issue API exists\n\n---\n\n**Confidence Assessment:** high — Exhaustive search of router confirms no issue endpoints."
    },
    {
      "id": "event-log-pipeline",
      "intent": "Understanding how events flow through the system tied to issues",
      "keywords": ["EventLog", "event", "pipeline", "queue", "IssueID", "Redis", "ingest", "async"],
      "scope": ["model/event_log.go", "service/event_ingest.go"],
      "report": "## Answer\nEvents are ingested via `EventIngestService`, creating `EventLog` records with `IssueID`, then enqueued to Redis for async processing. This pattern could be extended for token tracking.\n\n## Evidence\n- model/event_log.go:8-21 — EventLog has WorkspaceID, IssueID, EventType, Payload, processing fields\n- service/event_ingest.go:212-255 — CreateOrGet EventLog then enqueue to Redis with IssueID/EventLogID\n\n## Snippets\n```go\n// model/event_log.go\ntype EventLog struct {\n    ID          int64\n    WorkspaceID int64\n    IssueID     *int64\n    EventType   string\n    Payload     json.RawMessage\n}\n```\n\n## Confidence\nmedium — Pattern exists but may not directly apply to token tracking\n\n---\n\n**Confidence Assessment:** medium — The event pipeline is issue-centric but token tracking might need a simpler approach."
    },
    {
      "id": "llm-client-wrapper",
      "intent": "Understanding how LLM providers are called and what wrapper/helper is used for API calls",
      "keywords": ["LLM", "openai", "anthropic", "client", "wrapper", "ChatWithTools", "AgentClient", "API", "chat.completions", "provider"],
      "scope": ["common/llm/llm.go", "common/llm/openai.go", "common/llm/anthropic.go"],
      "report": "## Answer\nLLM calls are made through the `AgentClient` interface in `common/llm/llm.go`. The primary implementation is `openaiClient` in `common/llm/openai.go` which wraps the official OpenAI Go SDK. Token usage (PromptTokens, CompletionTokens, ReasoningTokens) is returned in the `AgentResponse` struct.\n\n## Evidence\n- common/llm/llm.go:15-22 — `AgentClient` interface with `ChatWithTools(ctx, AgentRequest) (*AgentResponse, error)` method\n- common/llm/llm.go:53-59 — `AgentResponse` struct has `PromptTokens int`, `CompletionTokens int`, `ReasoningTokens int`\n- common/llm/openai.go:53-117 — `openaiClient.ChatWithTools` makes the actual API call via `c.client.Chat.Completions.New()`\n- common/llm/openai.go:100-106 — Token usage extracted: `PromptTokens: int(resp.Usage.PromptTokens)`, `CompletionTokens: int(resp.Usage.CompletionTokens)`\n- common/llm/llm.go:32-45 — `NewAgentClient` factory creates provider-specific client based on `cfg.Provider`\n\n## Snippets\n```go\n// common/llm/llm.go:15-22\ntype AgentClient interface {\n    ChatWithTools(ctx context.Context, req AgentRequest) (*AgentResponse, error)\n    Model() string\n}\n\n// common/llm/llm.go:53-59\ntype AgentResponse struct {\n    Content          string\n    ToolCalls        []ToolCall\n    FinishReason     string\n    PromptTokens     int\n    CompletionTokens int\n    ReasoningTokens  int\n}\n\n// common/llm/openai.go:100-106\nresult := &AgentResponse{\n    Content:          choice.Message.Content,\n    FinishReason:     string(choice.FinishReason),\n    PromptTokens:     int(resp.Usage.PromptTokens),\n    CompletionTokens: int(resp.Usage.CompletionTokens),\n    ReasoningTokens:  int(resp.Usage.CompletionTokensDetails.ReasoningTokens),\n}\n```\n\n## Confidence\nhigh — Direct code inspection of the LLM abstraction layer\n\n---\n\n**Confidence Assessment:** high — The AgentClient interface and openaiClient implementation are the canonical way to make LLM calls."
    },
    {
      "id": "planner-metrics-structure",
      "intent": "Understanding PlannerMetrics and PlannerOutput structures for token tracking",
      "keywords": ["PlannerMetrics", "PlannerOutput", "metrics", "tokens", "totalPromptTokens", "totalCompletionTokens", "actions", "debug", "writeMetricsLog"],
      "scope": ["brain/planner.go"],
      "report": "## Answer\nPlanner has two key structures: `PlannerOutput` (returned to caller, no tokens) and `PlannerMetrics` (written to debug logs, includes tokens). Token counts ARE tracked internally but NOT returned to orchestrator.\n\n## Evidence\n- brain/planner.go:42-47 — `PlannerOutput` struct has Actions, Reasoning, Messages, LastToolCallID but NO token fields\n- brain/planner.go:51-75 — `PlannerMetrics` struct has `TotalPromptTokens int` (line 58) and `TotalCompletionTokens int` (line 59)\n- brain/planner.go:132-133 — Token accumulator initialization: `totalPromptTokens := 0`, `totalCompletionTokens := 0`\n- brain/planner.go:162-163 — Per-iteration accumulation: `totalPromptTokens += resp.PromptTokens`, `totalCompletionTokens += resp.CompletionTokens`\n- brain/planner.go:208-209 — Stored in metrics: `metrics.TotalPromptTokens = totalPromptTokens`\n- brain/planner.go:211 — Written to disk only: `p.writeMetricsLog(metrics)` - NOT returned to orchestrator\n- brain/planner.go:226-231 — Return PlannerOutput WITHOUT token information\n\n## Snippets\n```go\n// brain/planner.go:42-47 - OUTPUT (no tokens)\ntype PlannerOutput struct {\n    Actions        []Action\n    Reasoning      string\n    Messages       []llm.Message\n    LastToolCallID string\n}\n\n// brain/planner.go:51-75 - METRICS (has tokens)\ntype PlannerMetrics struct {\n    SessionID             string\n    IssueID               int64\n    TotalPromptTokens     int  // line 58\n    TotalCompletionTokens int  // line 59\n    // ... more fields\n}\n\n// brain/planner.go:162-163 - ACCUMULATION\ntotalPromptTokens += resp.PromptTokens\ntotalCompletionTokens += resp.CompletionTokens\n```\n\n## Confidence\nhigh — PlannerOutput needs token fields to pass tokens to orchestrator\n\n---\n\n**Confidence Assessment:** high — The gap between PlannerMetrics (has tokens) and PlannerOutput (no tokens) is the root cause."
    },
    {
      "id": "explore-agent-metrics",
      "intent": "Understanding ExploreAgent metrics and token tracking in explore calls",
      "keywords": ["ExploreAgent", "ExploreMetrics", "explore", "ContextWindowTokens", "TotalCompletionTokens", "thoroughness", "token limits"],
      "scope": ["brain/explore_agent.go"],
      "report": "## Answer\nExploreAgent tracks tokens via `ExploreMetrics` struct and has configurable token limits per thoroughness level. Metrics are written to debug logs but NOT returned to the caller (Planner).\n\n## Evidence\n- brain/explore_agent.go:67-96 — `ExploreMetrics` struct with `ContextWindowTokens int` (line 76) and `TotalCompletionTokens int` (line 77)\n- brain/explore_agent.go:42-65 — `ThoroughnessConfig` defines token limits per level:\n  - Quick: SoftTokenTarget=15000, HardTokenLimit=25000\n  - Medium: SoftTokenTarget=25000, HardTokenLimit=40000\n  - Thorough: SoftTokenTarget=60000, HardTokenLimit=100000\n- brain/explore_agent.go:192-197 — Token tracking initialization in Explore()\n- brain/explore_agent.go:287-288 — Per-iteration: `contextWindowTokens = resp.PromptTokens`, `totalCompletionTokens += resp.CompletionTokens`\n- brain/explore_agent.go:202-222 — Metrics written via `e.writeMetricsLog(metrics)` in defer, NOT passed back\n\n## Snippets\n```go\n// brain/explore_agent.go:67-96\ntype ExploreMetrics struct {\n    SessionID             string\n    Query                 string\n    Thoroughness          string\n    Iterations            int\n    ContextWindowTokens   int  // line 76 - last context size\n    TotalCompletionTokens int  // line 77 - sum of all completions\n    ToolCalls             map[string]int\n    Confidence            string\n    TerminationReason     string\n}\n\n// brain/explore_agent.go:42-65 - Thoroughness configs\ncase ThoughnessMedium:\n    return ThoroughnessConfig{\n        MaxIterations:   50,\n        SoftTokenTarget: 25000,\n        HardTokenLimit:  40000,\n    }\n```\n\n## Confidence\nhigh — ExploreMetrics are comprehensive but not propagated back to Planner\n\n---\n\n**Confidence Assessment:** high — Token tracking exists but is siloed in debug logs."
    },
    {
      "id": "database-schema-llm-evals",
      "intent": "Understanding llm_evals table schema and what columns store token data",
      "keywords": ["llm_evals", "schema", "database", "prompt_tokens", "completion_tokens", "migration", "table", "stage"],
      "scope": ["migrations/20251206181235_init_schema.sql", "core/db/sqlc/models.go"],
      "report": "## Answer\nThe `llm_evals` table has `prompt_tokens` and `completion_tokens` columns and is keyed by `issue_id` and `stage`. The infrastructure exists for per-issue token aggregation.\n\n## Evidence\n- migrations/20251206181235_init_schema.sql:281-319 — `llm_evals` table definition with token columns:\n  - Line 300: `prompt_tokens int`\n  - Line 301: `completion_tokens int`\n  - Line 284: `issue_id bigint references issues(id)` — links to issue\n  - Line 287: `stage text not null` — pipeline stage (keywords, planner, gap_detector, spec_generator)\n- migrations/20251206181235_init_schema.sql:316-319 — Indexes include `idx_llm_evals_issue_id` for efficient per-issue queries\n- core/db/sqlc/models.go:123-143 — Generated `LlmEval` struct has `PromptTokens *int32` and `CompletionTokens *int32`\n\n## Snippets\n```sql\n-- migrations/20251206181235_init_schema.sql:281-319\ncreate table llm_evals (\n    id bigint primary key,\n    workspace_id bigint references workspaces(id),\n    issue_id bigint references issues(id),\n    stage text not null,\n    input_text text not null,\n    output_json jsonb not null,\n    model text not null,\n    latency_ms int,\n    prompt_tokens int,      -- line 300\n    completion_tokens int,  -- line 301\n    created_at timestamptz not null default now()\n);\n\ncreate index idx_llm_evals_issue_id on llm_evals(issue_id);\n```\n\n## Confidence\nhigh — Database schema supports token tracking per issue\n\n---\n\n**Confidence Assessment:** high — Schema is ready, just needs to be used by planner/orchestrator."
    },
    {
      "id": "sqlc-queries-llm-evals",
      "intent": "Understanding SQLC queries for llm_evals and what aggregation exists",
      "keywords": ["SQLC", "queries", "InsertLLMEval", "GetEvalStats", "ListLLMEvalsByIssue", "aggregation", "SUM", "AVG"],
      "scope": ["core/db/queries/llm_evals.sql", "core/db/sqlc/llm_evals.sql.go"],
      "report": "## Answer\nSQLC queries exist for inserting evals and listing by issue, but `GetEvalStats` only aggregates by stage (not by issue) and doesn't include token sums. A new query is needed for per-issue token aggregation.\n\n## Evidence\n- core/db/queries/llm_evals.sql:1-14 — `InsertLLMEval` inserts token counts\n- core/db/queries/llm_evals.sql:19-22 — `ListLLMEvalsByIssue` returns all evals for an issue (no aggregation)\n- core/db/queries/llm_evals.sql:51-62 — `GetEvalStats` aggregates by stage:\n  - Only computes `AVG(latency_ms)` as `avg_latency_ms`\n  - NO `SUM(prompt_tokens)` or `SUM(completion_tokens)`\n  - Groups by stage only, not by issue\n\n## Snippets\n```sql\n-- core/db/queries/llm_evals.sql:51-62\n-- name: GetEvalStats :one\nSELECT\n    stage,\n    COUNT(*) as total,\n    COUNT(rating) as rated,\n    AVG(rating)::float as avg_rating,\n    AVG(eval_score)::float as avg_eval_score,\n    AVG(latency_ms)::int as avg_latency_ms  -- NO TOKEN SUMS\nFROM llm_evals\nWHERE stage = $1\n  AND created_at > $2\nGROUP BY stage;\n\n-- MISSING: GetTokenConsumptionByIssue\n-- SELECT issue_id, SUM(prompt_tokens), SUM(completion_tokens)\n-- FROM llm_evals WHERE issue_id = $1 GROUP BY issue_id;\n```\n\n## Confidence\nhigh — Queries need extension for per-issue token aggregation\n\n---\n\n**Confidence Assessment:** high — Schema exists but aggregation query is missing."
    },
    {
      "id": "action-executor-flow",
      "intent": "Understanding how actions are executed after planning and where token persistence could be added",
      "keywords": ["ActionExecutor", "ExecuteBatch", "actions", "WriteGaps", "WriteKeywords", "UpdateFindings", "UpdateLearnings"],
      "scope": ["brain/action_executor.go", "brain/action.go"],
      "report": "## Answer\nActionExecutor.ExecuteBatch() executes planner actions (post_comment, update_findings, update_gaps, update_learnings) after planning completes. No token tracking happens here. The orchestrator flow after action execution is the ideal place to persist aggregated tokens.\n\n## Evidence\n- brain/action_executor.go:27-41 — `NewActionExecutor` takes stores for issues, gaps, integrations, learnings\n- brain/action_executor.go:59-67 — `ExecuteBatch(ctx, issue, actions)` executes actions sequentially\n- brain/action.go:14-20 — Action types: `post_comment`, `update_findings`, `update_gaps`, `update_learnings`, `ready_for_spec`\n- brain/orchestrator.go:396 — `errs := executor.ExecuteBatch(ctx, *issue, output.Actions)` — called after planning\n- brain/orchestrator.go:396-405 — After ExecuteBatch is where token persistence should be added\n\n## Snippets\n```go\n// brain/action_executor.go:59-67\nfunc (e *actionExecutor) ExecuteBatch(ctx context.Context, issue model.Issue, actions []Action) []ActionError {\n    var errs []ActionError\n    for _, action := range actions {\n        if err := e.execute(ctx, issue, action); err != nil {\n            errs = append(errs, ActionError{Action: action, Err: err})\n        }\n    }\n    return errs\n}\n\n// brain/orchestrator.go:396 — IDEAL TOKEN PERSISTENCE POINT\nerrs := executor.ExecuteBatch(ctx, *issue, output.Actions)\n// After this: persist output.TotalPromptTokens, output.TotalCompletionTokens\n```\n\n## Confidence\nhigh — Action execution flow is clear, token persistence fits after ExecuteBatch\n\n---\n\n**Confidence Assessment:** high — The orchestrator.runPlannerCycle() after line 396 is the integration point."
    },
    {
      "id": "gap-model-structure",
      "intent": "Understanding Gap model and whether it tracks any token usage",
      "keywords": ["Gap", "model", "struct", "fields", "IssueID", "Status", "Question", "Severity", "no tokens"],
      "scope": ["model/gap.go", "store/gap.go"],
      "report": "## Answer\nThe Gap model stores information gaps discovered during issue analysis. It does NOT track token usage - only gap metadata (question, evidence, severity, status).\n\n## Evidence\n- model/gap.go:29-43 — Gap struct fields:\n  - ID, IssueID, Status, Question, Evidence, Severity, Respondent\n  - CreatedAt, ResolvedAt\n  - NO token consumption fields\n- store/gap.go — Basic CRUD operations, no token-related methods\n\n## Snippets\n```go\n// model/gap.go:29-43\ntype Gap struct {\n    ID          int64      `json:\"id\"`\n    IssueID     int64      `json:\"issue_id\"`\n    Status      GapStatus  `json:\"status\"`\n    Question    string     `json:\"question\"`\n    Evidence    string     `json:\"evidence,omitempty\"`\n    Severity    GapSeverity `json:\"severity\"`\n    Respondent  *string    `json:\"respondent,omitempty\"`\n    CreatedAt   time.Time  `json:\"created_at\"`\n    ResolvedAt  *time.Time `json:\"resolved_at,omitempty\"`\n    // NO TOKEN FIELDS\n}\n```\n\n## Confidence\nhigh — Gap model is unrelated to token tracking\n\n---\n\n**Confidence Assessment:** high — Gap model is for information gaps, not LLM tokens."
    },
    {
      "id": "learning-model-structure",
      "intent": "Understanding Learning model and whether it stores any token data",
      "keywords": ["Learning", "model", "struct", "fields", "WorkspaceID", "Type", "Content", "RuleUpdatedByIssueID"],
      "scope": ["model/learning.go", "store/learning.go"],
      "report": "## Answer\nThe Learning model stores workspace-level learnings (rules, patterns) extracted from issues. It does NOT track token usage.\n\n## Evidence\n- model/learning.go:5-13 — Learning struct fields:\n  - ID, ShortID, WorkspaceID, RuleUpdatedByIssueID\n  - Type, Content\n  - CreatedAt, UpdatedAt\n  - NO token consumption fields\n- store/learning.go — CRUD and list operations only, no token aggregation\n\n## Snippets\n```go\n// model/learning.go:5-13\ntype Learning struct {\n    ID                   int64     `json:\"id\"`\n    ShortID              string    `json:\"short_id\"`\n    WorkspaceID          int64     `json:\"workspace_id\"`\n    RuleUpdatedByIssueID *int64    `json:\"rule_updated_by_issue_id,omitempty\"`\n    Type                 string    `json:\"type\"`\n    Content              string    `json:\"content\"`\n    CreatedAt            time.Time `json:\"created_at\"`\n    UpdatedAt            time.Time `json:\"updated_at\"`\n    // NO TOKEN FIELDS\n}\n```\n\n## Confidence\nhigh — Learning model is for extracted rules, not token tracking\n\n---\n\n**Confidence Assessment:** high — Learning model is unrelated to token consumption."
    }
  ]
}
