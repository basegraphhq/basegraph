{
  "version": "1.0",
  "issue_context": "Token Consumption - Track token spent per issue",
  "fixtures": [
    {
      "id": "token-tracking-in-agents",
      "intent": "Understanding how tokens are currently tracked in planner and explore agents",
      "keywords": ["token", "tracking", "planner", "explore", "prompt_tokens", "completion_tokens", "accumulation", "usage"],
      "scope": ["brain/planner.go", "brain/explore_agent.go"],
      "report": "## Answer\nToken usage (prompt/completion) is accumulated in both Planner and ExploreAgent during their execution loops, but only logged - not persisted to database.\n\n## Evidence\n- brain/planner.go:113-125 — Planner accumulates `totalPromptTokens += resp.PromptTokens` and `totalCompletionTokens += resp.CompletionTokens` per iteration\n- brain/explore_agent.go:126-138 — ExploreAgent does the same accumulation pattern\n- brain/explore_agent.go:178-189 — Synthesis call also adds to totals\n\n## Snippets\n```go\n// planner.go:113-125\ntotalPromptTokens += resp.PromptTokens\ntotalCompletionTokens += resp.CompletionTokens\nslog.DebugContext(ctx, \"planner iteration completed\", \"prompt_tokens\", resp.PromptTokens)\n```\n\n## Confidence\nhigh — Direct code inspection confirms the pattern\n\n---\n\n**Confidence Assessment:** high — The token tracking logic is explicit and consistent across both agents."
    },
    {
      "id": "llm-eval-persistence",
      "intent": "Understanding LLMEval model and how it could persist token usage",
      "keywords": ["LLMEval", "persistence", "database", "store", "issue_id", "prompt_tokens", "completion_tokens", "save", "persist"],
      "scope": ["model/llm_eval.go", "store/llm_eval.go", "store/interfaces.go"],
      "report": "## Answer\nThere is an existing `LLMEval` model with `IssueID`, `PromptTokens`, and `CompletionTokens` fields, plus a store with `Create` and `ListByIssue` methods. However, no code currently calls the store to persist token usage.\n\n## Evidence\n- model/llm_eval.go:14-31 — `LLMEval` struct has `IssueID *int64`, `PromptTokens *int`, `CompletionTokens *int`\n- store/llm_eval.go:20-48 — `InsertLLMEval` persists these fields\n- store/interfaces.go:144-152 — `LLMEvalStore` interface supports `Create`, `ListByIssue`, `GetStats`\n\n## Snippets\n```go\n// model/llm_eval.go\ntype LLMEval struct {\n    ID               int64   `json:\"id\"`\n    IssueID          *int64  `json:\"issue_id,omitempty\"`\n    Stage            string  `json:\"stage\"`\n    PromptTokens     *int    `json:\"prompt_tokens,omitempty\"`\n    CompletionTokens *int    `json:\"completion_tokens,omitempty\"`\n}\n```\n\n## Confidence\nhigh — Model and store exist but are unused\n\n---\n\n**Confidence Assessment:** high — The infrastructure exists, just needs to be wired up."
    },
    {
      "id": "issue-model-structure",
      "intent": "Understanding Issue model and what fields it has",
      "keywords": ["Issue", "model", "struct", "fields", "CodeFindings", "Keywords", "Discussions", "schema"],
      "scope": ["model/issue.go", "store/issue.go"],
      "report": "## Answer\nThe `Issue` model stores issue metadata, discussions, keywords, code findings, and processing state. It does NOT currently have token usage fields.\n\n## Evidence\n- model/issue.go:83-109 — Issue struct with ID, IntegrationID, Title, Description, Labels, Keywords, CodeFindings, Discussions, Spec, ProcessingStatus\n- store/issue.go:21-61 — UpsertIssueParams marshals Keywords/CodeFindings/Learnings/Discussions as JSON\n\n## Snippets\n```go\n// model/issue.go:83-109\ntype Issue struct {\n    ID                int64         `json:\"id\"`\n    IntegrationID     int64         `json:\"integration_id\"`\n    Keywords          []Keyword     `json:\"keywords,omitempty\"`\n    CodeFindings      []CodeFinding `json:\"code_findings,omitempty\"`\n    Discussions       []Discussion  `json:\"discussions,omitempty\"`\n    ProcessingStatus  ProcessingStatus `json:\"processing_status\"`\n}\n```\n\n## Confidence\nhigh — Comprehensive view of Issue model\n\n---\n\n**Confidence Assessment:** high — Direct struct inspection."
    },
    {
      "id": "orchestrator-engagement-flow",
      "intent": "Understanding how orchestrator processes issues and where token tracking could be added",
      "keywords": ["orchestrator", "HandleEngagement", "IssueID", "EventLogID", "planner", "context", "flow", "processing"],
      "scope": ["brain/orchestrator.go"],
      "report": "## Answer\nOrchestrator's `HandleEngagement` loads an issue, runs the planner, and executes actions. IssueID is available in context throughout. This is the ideal place to persist aggregated token counts after planning completes.\n\n## Evidence\n- brain/orchestrator.go:19-24 — `EngagementInput` has `IssueID int64`, `EventLogID int64`\n- brain/orchestrator.go:94-100 — Context enriched with IssueID via `logger.WithLogFields`\n- brain/orchestrator.go:134-136 — `output, err := o.planner.Plan(ctx, messages)` is where planning happens\n- brain/orchestrator.go:94-165 — After planning, actions are executed, then function returns (end-of-processing hook point)\n\n## Snippets\n```go\n// brain/orchestrator.go:94-100\nctx = logger.WithLogFields(ctx, logger.LogFields{\n    IssueID:    &input.IssueID,\n    EventLogID: &input.EventLogID,\n})\n```\n\n## Confidence\nhigh — Clear insertion point identified\n\n---\n\n**Confidence Assessment:** high — The orchestrator flow is well-defined and IssueID is available."
    },
    {
      "id": "http-api-routes",
      "intent": "Understanding what HTTP API routes exist and whether there's an issue endpoint",
      "keywords": ["HTTP", "API", "routes", "router", "issue", "endpoint", "v1", "REST"],
      "scope": ["http/router/router.go", "http/dto"],
      "report": "## Answer\nThe HTTP API currently has `/api/v1/users`, `/api/v1/organizations`, and `/api/v1/integrations/gitlab` routes. There is NO `/api/v1/issues` endpoint yet.\n\n## Evidence\n- http/router/router.go:25-35 — Only registers users/orgs/gitlab routes under `/api/v1`\n- http/dto/*.go — DTO patterns exist for user/org but no IssueResponse DTO\n- http/middleware/auth.go:22-24 — TODO comment mentions auth middleware not applied to v1 routes yet\n\n## Snippets\n```go\n// http/router/router.go:17-35\nv1 := router.Group(\"/api/v1\") {\n    // users, organizations, gitlab...\n    // NO issues route\n}\n```\n\n## Confidence\nhigh — No issue API exists\n\n---\n\n**Confidence Assessment:** high — Exhaustive search of router confirms no issue endpoints."
    },
    {
      "id": "event-log-pipeline",
      "intent": "Understanding how events flow through the system tied to issues",
      "keywords": ["EventLog", "event", "pipeline", "queue", "IssueID", "Redis", "ingest", "async"],
      "scope": ["model/event_log.go", "service/event_ingest.go"],
      "report": "## Answer\nEvents are ingested via `EventIngestService`, creating `EventLog` records with `IssueID`, then enqueued to Redis for async processing. This pattern could be extended for token tracking.\n\n## Evidence\n- model/event_log.go:8-21 — EventLog has WorkspaceID, IssueID, EventType, Payload, processing fields\n- service/event_ingest.go:212-255 — CreateOrGet EventLog then enqueue to Redis with IssueID/EventLogID\n\n## Snippets\n```go\n// model/event_log.go\ntype EventLog struct {\n    ID          int64\n    WorkspaceID int64\n    IssueID     *int64\n    EventType   string\n    Payload     json.RawMessage\n}\n```\n\n## Confidence\nmedium — Pattern exists but may not directly apply to token tracking\n\n---\n\n**Confidence Assessment:** medium — The event pipeline is issue-centric but token tracking might need a simpler approach."
    },
    {
      "id": "llm-client-wrapper",
      "intent": "Understanding how LLM providers are called and what wrapper/helper is used for API calls",
      "keywords": ["LLM", "openai", "anthropic", "client", "wrapper", "ChatWithTools", "AgentClient", "API", "chat.completions", "provider"],
      "scope": ["common/llm/llm.go", "common/llm/openai.go", "common/llm/anthropic.go"],
      "report": "## Answer\nLLM calls are made through the `AgentClient` interface in `common/llm/llm.go`. The primary implementation is `openaiClient` in `common/llm/openai.go` which wraps the official OpenAI Go SDK. Token usage (PromptTokens, CompletionTokens, ReasoningTokens) is returned in the `AgentResponse` struct.\n\n## Evidence\n- common/llm/llm.go:15-22 — `AgentClient` interface with `ChatWithTools(ctx, AgentRequest) (*AgentResponse, error)` method\n- common/llm/llm.go:53-59 — `AgentResponse` struct has `PromptTokens int`, `CompletionTokens int`, `ReasoningTokens int`\n- common/llm/openai.go:53-117 — `openaiClient.ChatWithTools` makes the actual API call via `c.client.Chat.Completions.New()`\n- common/llm/openai.go:100-106 — Token usage extracted: `PromptTokens: int(resp.Usage.PromptTokens)`, `CompletionTokens: int(resp.Usage.CompletionTokens)`\n- common/llm/llm.go:32-45 — `NewAgentClient` factory creates provider-specific client based on `cfg.Provider`\n\n## Snippets\n```go\n// common/llm/llm.go:15-22\ntype AgentClient interface {\n    ChatWithTools(ctx context.Context, req AgentRequest) (*AgentResponse, error)\n    Model() string\n}\n\n// common/llm/llm.go:53-59\ntype AgentResponse struct {\n    Content          string\n    ToolCalls        []ToolCall\n    FinishReason     string\n    PromptTokens     int\n    CompletionTokens int\n    ReasoningTokens  int\n}\n\n// common/llm/openai.go:100-106\nresult := &AgentResponse{\n    Content:          choice.Message.Content,\n    FinishReason:     string(choice.FinishReason),\n    PromptTokens:     int(resp.Usage.PromptTokens),\n    CompletionTokens: int(resp.Usage.CompletionTokens),\n    ReasoningTokens:  int(resp.Usage.CompletionTokensDetails.ReasoningTokens),\n}\n```\n\n## Confidence\nhigh — Direct code inspection of the LLM abstraction layer\n\n---\n\n**Confidence Assessment:** high — The AgentClient interface and openaiClient implementation are the canonical way to make LLM calls."
    },
    {
      "id": "planner-metrics-structure",
      "intent": "Understanding PlannerMetrics and PlannerOutput structures for token tracking",
      "keywords": ["PlannerMetrics", "PlannerOutput", "metrics", "tokens", "totalPromptTokens", "totalCompletionTokens", "actions", "debug", "writeMetricsLog"],
      "scope": ["brain/planner.go"],
      "report": "## Answer\nPlanner has two key structures: `PlannerOutput` (returned to caller, no tokens) and `PlannerMetrics` (written to debug logs, includes tokens). Token counts ARE tracked internally but NOT returned to orchestrator.\n\n## Evidence\n- brain/planner.go:42-47 — `PlannerOutput` struct has Actions, Reasoning, Messages, LastToolCallID but NO token fields\n- brain/planner.go:51-75 — `PlannerMetrics` struct has `TotalPromptTokens int` (line 58) and `TotalCompletionTokens int` (line 59)\n- brain/planner.go:132-133 — Token accumulator initialization: `totalPromptTokens := 0`, `totalCompletionTokens := 0`\n- brain/planner.go:162-163 — Per-iteration accumulation: `totalPromptTokens += resp.PromptTokens`, `totalCompletionTokens += resp.CompletionTokens`\n- brain/planner.go:208-209 — Stored in metrics: `metrics.TotalPromptTokens = totalPromptTokens`\n- brain/planner.go:211 — Written to disk only: `p.writeMetricsLog(metrics)` - NOT returned to orchestrator\n- brain/planner.go:226-231 — Return PlannerOutput WITHOUT token information\n\n## Snippets\n```go\n// brain/planner.go:42-47 - OUTPUT (no tokens)\ntype PlannerOutput struct {\n    Actions        []Action\n    Reasoning      string\n    Messages       []llm.Message\n    LastToolCallID string\n}\n\n// brain/planner.go:51-75 - METRICS (has tokens)\ntype PlannerMetrics struct {\n    SessionID             string\n    IssueID               int64\n    TotalPromptTokens     int  // line 58\n    TotalCompletionTokens int  // line 59\n    // ... more fields\n}\n\n// brain/planner.go:162-163 - ACCUMULATION\ntotalPromptTokens += resp.PromptTokens\ntotalCompletionTokens += resp.CompletionTokens\n```\n\n## Confidence\nhigh — PlannerOutput needs token fields to pass tokens to orchestrator\n\n---\n\n**Confidence Assessment:** high — The gap between PlannerMetrics (has tokens) and PlannerOutput (no tokens) is the root cause."
    },
    {
      "id": "explore-agent-metrics",
      "intent": "Understanding ExploreAgent metrics and token tracking in explore calls",
      "keywords": ["ExploreAgent", "ExploreMetrics", "explore", "ContextWindowTokens", "TotalCompletionTokens", "thoroughness", "token limits"],
      "scope": ["brain/explore_agent.go"],
      "report": "## Answer\nExploreAgent tracks tokens via `ExploreMetrics` struct and has configurable token limits per thoroughness level. Metrics are written to debug logs but NOT returned to the caller (Planner).\n\n## Evidence\n- brain/explore_agent.go:67-96 — `ExploreMetrics` struct with `ContextWindowTokens int` (line 76) and `TotalCompletionTokens int` (line 77)\n- brain/explore_agent.go:42-65 — `ThoroughnessConfig` defines token limits per level:\n  - Quick: SoftTokenTarget=15000, HardTokenLimit=25000\n  - Medium: SoftTokenTarget=25000, HardTokenLimit=40000\n  - Thorough: SoftTokenTarget=60000, HardTokenLimit=100000\n- brain/explore_agent.go:192-197 — Token tracking initialization in Explore()\n- brain/explore_agent.go:287-288 — Per-iteration: `contextWindowTokens = resp.PromptTokens`, `totalCompletionTokens += resp.CompletionTokens`\n- brain/explore_agent.go:202-222 — Metrics written via `e.writeMetricsLog(metrics)` in defer, NOT passed back\n\n## Snippets\n```go\n// brain/explore_agent.go:67-96\ntype ExploreMetrics struct {\n    SessionID             string\n    Query                 string\n    Thoroughness          string\n    Iterations            int\n    ContextWindowTokens   int  // line 76 - last context size\n    TotalCompletionTokens int  // line 77 - sum of all completions\n    ToolCalls             map[string]int\n    Confidence            string\n    TerminationReason     string\n}\n\n// brain/explore_agent.go:42-65 - Thoroughness configs\ncase ThoughnessMedium:\n    return ThoroughnessConfig{\n        MaxIterations:   50,\n        SoftTokenTarget: 25000,\n        HardTokenLimit:  40000,\n    }\n```\n\n## Confidence\nhigh — ExploreMetrics are comprehensive but not propagated back to Planner\n\n---\n\n**Confidence Assessment:** high — Token tracking exists but is siloed in debug logs."
    },
    {
      "id": "database-schema-llm-evals",
      "intent": "Understanding llm_evals table schema and what columns store token data",
      "keywords": ["llm_evals", "schema", "database", "prompt_tokens", "completion_tokens", "migration", "table", "stage"],
      "scope": ["migrations/20251206181235_init_schema.sql", "core/db/sqlc/models.go"],
      "report": "## Answer\nThe `llm_evals` table has `prompt_tokens` and `completion_tokens` columns and is keyed by `issue_id` and `stage`. The infrastructure exists for per-issue token aggregation.\n\n## Evidence\n- migrations/20251206181235_init_schema.sql:281-319 — `llm_evals` table definition with token columns:\n  - Line 300: `prompt_tokens int`\n  - Line 301: `completion_tokens int`\n  - Line 284: `issue_id bigint references issues(id)` — links to issue\n  - Line 287: `stage text not null` — pipeline stage (keywords, planner, gap_detector, spec_generator)\n- migrations/20251206181235_init_schema.sql:316-319 — Indexes include `idx_llm_evals_issue_id` for efficient per-issue queries\n- core/db/sqlc/models.go:123-143 — Generated `LlmEval` struct has `PromptTokens *int32` and `CompletionTokens *int32`\n\n## Snippets\n```sql\n-- migrations/20251206181235_init_schema.sql:281-319\ncreate table llm_evals (\n    id bigint primary key,\n    workspace_id bigint references workspaces(id),\n    issue_id bigint references issues(id),\n    stage text not null,\n    input_text text not null,\n    output_json jsonb not null,\n    model text not null,\n    latency_ms int,\n    prompt_tokens int,      -- line 300\n    completion_tokens int,  -- line 301\n    created_at timestamptz not null default now()\n);\n\ncreate index idx_llm_evals_issue_id on llm_evals(issue_id);\n```\n\n## Confidence\nhigh — Database schema supports token tracking per issue\n\n---\n\n**Confidence Assessment:** high — Schema is ready, just needs to be used by planner/orchestrator."
    },
    {
      "id": "sqlc-queries-llm-evals",
      "intent": "Understanding SQLC queries for llm_evals SQL definitions and what aggregation exists",
      "keywords": ["SQLC", "queries", "GetEvalStats", "ListLLMEvalsByIssue", "aggregation", "SUM", "AVG", "llm_evals.sql", "SQL definition"],
      "scope": ["core/db/queries/llm_evals.sql", "core/db/sqlc/llm_evals.sql.go"],
      "report": "## Answer\nSQLC queries exist for inserting evals and listing by issue, but `GetEvalStats` only aggregates by stage (not by issue) and doesn't include token sums. A new query is needed for per-issue token aggregation.\n\n## Evidence\n- core/db/queries/llm_evals.sql:1-14 — `InsertLLMEval` inserts token counts\n- core/db/queries/llm_evals.sql:19-22 — `ListLLMEvalsByIssue` returns all evals for an issue (no aggregation)\n- core/db/queries/llm_evals.sql:51-62 — `GetEvalStats` aggregates by stage:\n  - Only computes `AVG(latency_ms)` as `avg_latency_ms`\n  - NO `SUM(prompt_tokens)` or `SUM(completion_tokens)`\n  - Groups by stage only, not by issue\n\n## Snippets\n```sql\n-- core/db/queries/llm_evals.sql:51-62\n-- name: GetEvalStats :one\nSELECT\n    stage,\n    COUNT(*) as total,\n    COUNT(rating) as rated,\n    AVG(rating)::float as avg_rating,\n    AVG(eval_score)::float as avg_eval_score,\n    AVG(latency_ms)::int as avg_latency_ms  -- NO TOKEN SUMS\nFROM llm_evals\nWHERE stage = $1\n  AND created_at > $2\nGROUP BY stage;\n\n-- MISSING: GetTokenConsumptionByIssue\n-- SELECT issue_id, SUM(prompt_tokens), SUM(completion_tokens)\n-- FROM llm_evals WHERE issue_id = $1 GROUP BY issue_id;\n```\n\n## Confidence\nhigh — Queries need extension for per-issue token aggregation\n\n---\n\n**Confidence Assessment:** high — Schema exists but aggregation query is missing."
    },
    {
      "id": "insert-llm-eval-required-params",
      "intent": "Show InsertLLMEval SQL and required parameters",
      "keywords": ["InsertLLMEval", "llm_evals", "SQL", "parameters", "workspace_id", "issue_id", "stage", "input_text", "output_json", "model", "temperature", "prompt_version", "latency_ms", "prompt_tokens", "completion_tokens", "NOW()"],
      "scope": ["relay/core/db/queries/llm_evals.sql", "core/db/queries/llm_evals.sql", "core/db/sqlc/llm_evals.sql.go"],
      "report": "## Answer\n`InsertLLMEval` inserts a single row into `llm_evals` and returns the inserted row. It takes 12 SQL parameters ($1..$12); `created_at` is set via `NOW()` (no input parameter).\n\n## SQL (InsertLLMEval)\n```sql\n-- name: InsertLLMEval :one\nINSERT INTO llm_evals (\n    id, workspace_id, issue_id, stage,\n    input_text, output_json,\n    model, temperature, prompt_version,\n    latency_ms, prompt_tokens, completion_tokens,\n    created_at\n) VALUES (\n    $1, $2, $3, $4,\n    $5, $6,\n    $7, $8, $9,\n    $10, $11, $12,\n    NOW()\n) RETURNING *;\n```\n\n## Parameters\n- `$1` `id` (required)\n- `$2` `workspace_id` (nullable)\n- `$3` `issue_id` (nullable)\n- `$4` `stage` (required)\n- `$5` `input_text` (required)\n- `$6` `output_json` (required)\n- `$7` `model` (required)\n- `$8` `temperature` (nullable)\n- `$9` `prompt_version` (nullable)\n- `$10` `latency_ms` (nullable)\n- `$11` `prompt_tokens` (nullable)\n- `$12` `completion_tokens` (nullable)\n\n## Confidence\nhigh — Direct inspection of `relay/core/db/queries/llm_evals.sql`."
    },
    {
      "id": "action-executor-flow",
      "intent": "Understanding how actions are executed after planning and where token persistence could be added",
      "keywords": ["ActionExecutor", "ExecuteBatch", "actions", "WriteGaps", "WriteKeywords", "UpdateFindings", "UpdateLearnings"],
      "scope": ["brain/action_executor.go", "brain/action.go"],
      "report": "## Answer\nActionExecutor.ExecuteBatch() executes planner actions (post_comment, update_findings, update_gaps, update_learnings) after planning completes. No token tracking happens here. The orchestrator flow after action execution is the ideal place to persist aggregated tokens.\n\n## Evidence\n- brain/action_executor.go:27-41 — `NewActionExecutor` takes stores for issues, gaps, integrations, learnings\n- brain/action_executor.go:59-67 — `ExecuteBatch(ctx, issue, actions)` executes actions sequentially\n- brain/action.go:14-20 — Action types: `post_comment`, `update_findings`, `update_gaps`, `update_learnings`, `ready_for_spec`\n- brain/orchestrator.go:396 — `errs := executor.ExecuteBatch(ctx, *issue, output.Actions)` — called after planning\n- brain/orchestrator.go:396-405 — After ExecuteBatch is where token persistence should be added\n\n## Snippets\n```go\n// brain/action_executor.go:59-67\nfunc (e *actionExecutor) ExecuteBatch(ctx context.Context, issue model.Issue, actions []Action) []ActionError {\n    var errs []ActionError\n    for _, action := range actions {\n        if err := e.execute(ctx, issue, action); err != nil {\n            errs = append(errs, ActionError{Action: action, Err: err})\n        }\n    }\n    return errs\n}\n\n// brain/orchestrator.go:396 — IDEAL TOKEN PERSISTENCE POINT\nerrs := executor.ExecuteBatch(ctx, *issue, output.Actions)\n// After this: persist output.TotalPromptTokens, output.TotalCompletionTokens\n```\n\n## Confidence\nhigh — Action execution flow is clear, token persistence fits after ExecuteBatch\n\n---\n\n**Confidence Assessment:** high — The orchestrator.runPlannerCycle() after line 396 is the integration point."
    },
    {
      "id": "gap-model-structure",
      "intent": "Understanding Gap model and whether it tracks any token usage",
      "keywords": ["Gap", "model", "struct", "fields", "IssueID", "Status", "Question", "Severity", "no tokens"],
      "scope": ["model/gap.go", "store/gap.go"],
      "report": "## Answer\nThe Gap model stores information gaps discovered during issue analysis. It does NOT track token usage - only gap metadata (question, evidence, severity, status).\n\n## Evidence\n- model/gap.go:29-43 — Gap struct fields:\n  - ID, IssueID, Status, Question, Evidence, Severity, Respondent\n  - CreatedAt, ResolvedAt\n  - NO token consumption fields\n- store/gap.go — Basic CRUD operations, no token-related methods\n\n## Snippets\n```go\n// model/gap.go:29-43\ntype Gap struct {\n    ID          int64      `json:\"id\"`\n    IssueID     int64      `json:\"issue_id\"`\n    Status      GapStatus  `json:\"status\"`\n    Question    string     `json:\"question\"`\n    Evidence    string     `json:\"evidence,omitempty\"`\n    Severity    GapSeverity `json:\"severity\"`\n    Respondent  *string    `json:\"respondent,omitempty\"`\n    CreatedAt   time.Time  `json:\"created_at\"`\n    ResolvedAt  *time.Time `json:\"resolved_at,omitempty\"`\n    // NO TOKEN FIELDS\n}\n```\n\n## Confidence\nhigh — Gap model is unrelated to token tracking\n\n---\n\n**Confidence Assessment:** high — Gap model is for information gaps, not LLM tokens."
    },
    {
      "id": "learning-model-structure",
      "intent": "Understanding Learning model and whether it stores any token data",
      "keywords": ["Learning", "model", "struct", "fields", "WorkspaceID", "Type", "Content", "RuleUpdatedByIssueID"],
      "scope": ["model/learning.go", "store/learning.go"],
      "report": "## Answer\nThe Learning model stores workspace-level learnings (rules, patterns) extracted from issues. It does NOT track token usage.\n\n## Evidence\n- model/learning.go:5-13 — Learning struct fields:\n  - ID, ShortID, WorkspaceID, RuleUpdatedByIssueID\n  - Type, Content\n  - CreatedAt, UpdatedAt\n  - NO token consumption fields\n- store/learning.go — CRUD and list operations only, no token aggregation\n\n## Snippets\n```go\n// model/learning.go:5-13\ntype Learning struct {\n    ID                   int64     `json:\"id\"`\n    ShortID              string    `json:\"short_id\"`\n    WorkspaceID          int64     `json:\"workspace_id\"`\n    RuleUpdatedByIssueID *int64    `json:\"rule_updated_by_issue_id,omitempty\"`\n    Type                 string    `json:\"type\"`\n    Content              string    `json:\"content\"`\n    CreatedAt            time.Time `json:\"created_at\"`\n    UpdatedAt            time.Time `json:\"updated_at\"`\n    // NO TOKEN FIELDS\n}\n```\n\n## Confidence\nhigh — Learning model is for extracted rules, not token tracking\n\n---\n\n**Confidence Assessment:** high — Learning model is unrelated to token consumption."
    },
    {
      "id": "non-http-apis-issue",
      "intent": "Understanding whether there are any non-HTTP APIs (gRPC, GraphQL, resolvers) exposing Issue",
      "keywords": ["grpc", "graphql", "resolver", "IssueService", "IssueHandler", "GetIssue", "non-HTTP", "API", "service"],
      "scope": ["service/", "graphql/", "grpc/", "resolver/"],
      "report": "## Answer\nThere are NO non-HTTP APIs (gRPC, GraphQL, resolvers) exposing Issue in this codebase. The relay service uses HTTP REST endpoints only. Issue data flows through the orchestrator/planner internally but is not exposed via any RPC interface.\n\n## Evidence\n- No grpc/ or graphql/ directories exist in the codebase\n- No .proto files or GraphQL schema files found\n- service/ directory contains internal services (event_ingest, issue_tracker) but no RPC handlers\n- Issue is only accessed internally via store.IssueStore interface\n- http/router/router.go — Only HTTP routes exist, no gRPC or GraphQL registration\n\n## Snippets\n```\n# Directory structure - no RPC frameworks\nrelay/\n├── cmd/           # CLI entrypoints (server, worker)\n├── common/        # Shared utilities (llm, arangodb)\n├── core/          # Database layer (db, config)\n├── http/          # HTTP handlers only\n├── internal/      # Business logic\n│   ├── brain/     # Orchestrator, planner (internal)\n│   ├── model/     # Domain models\n│   ├── service/   # Internal services\n│   └── store/     # Database access\n└── migrations/\n\n# No gRPC/GraphQL:\n- No *.proto files\n- No graphql/ or resolver/ directories\n- No gqlgen.yml or schema.graphqls\n```\n\n## Confidence\nhigh — Exhaustive search confirms HTTP-only architecture\n\n---\n\n**Confidence Assessment:** high — The codebase uses REST HTTP exclusively, no gRPC or GraphQL."
    },
    {
      "id": "issue-sql-queries",
      "intent": "Understanding SQL queries for issues and what UPDATE methods exist",
      "keywords": ["SQL", "issues", "UPDATE", "queries", "UpsertIssue", "SetProcessingStatus", "atomic", "increment", "issues.sql"],
      "scope": ["core/db/queries/issues.sql", "core/db/sqlc/issues.sql.go", "store/issue.go"],
      "report": "## Answer\nIssue SQL queries use UPSERT pattern for full updates and specific methods for processing status. There is NO atomic increment UPDATE for token fields - the issues table doesn't have token columns.\n\n## Evidence\n- core/db/queries/issues.sql:1-30 — `UpsertIssue` does full INSERT...ON CONFLICT UPDATE for all fields\n- core/db/queries/issues.sql:32-45 — `GetIssueByID` fetches single issue\n- core/db/queries/issues.sql:47-60 — `SetProcessingStatus` updates only processing_status field\n- core/db/queries/issues.sql:62-75 — `ListIssuesByIntegration` with pagination\n- migrations/20251206181235_init_schema.sql:149-187 — issues table has NO token columns (prompt_tokens, completion_tokens)\n\n## Snippets\n```sql\n-- core/db/queries/issues.sql - UpsertIssue (simplified)\n-- name: UpsertIssue :one\nINSERT INTO issues (\n    id, integration_id, external_id, title, description,\n    labels, keywords, code_findings, learnings, discussions,\n    spec, processing_status, created_at, updated_at\n) VALUES (...)\nON CONFLICT (integration_id, external_id) DO UPDATE SET\n    title = EXCLUDED.title,\n    description = EXCLUDED.description,\n    -- ... all fields updated\nRETURNING *;\n\n-- SetProcessingStatus - targeted update\n-- name: SetProcessingStatus :exec\nUPDATE issues SET\n    processing_status = $2,\n    updated_at = now()\nWHERE id = $1;\n\n-- NO atomic increment like:\n-- UPDATE issues SET prompt_tokens = prompt_tokens + $2 WHERE id = $1\n```\n\n## Confidence\nhigh — Issues table lacks token columns, so no atomic increment exists\n\n---\n\n**Confidence Assessment:** high — Token tracking needs llm_evals table, not issues table."
    },
    {
      "id": "chatwithtools-call-sites",
      "intent": "Understanding where AgentClient.ChatWithTools is called from and what context is available",
      "keywords": ["ChatWithTools", "AgentClient", "call sites", "planner", "explore", "IssueID", "context", "caller"],
      "scope": ["brain/planner.go", "brain/explore_agent.go", "brain/orchestrator.go"],
      "report": "## Answer\nAgentClient.ChatWithTools is called from two locations: Planner.Plan() and ExploreAgent.Explore(). Both have access to IssueID through context (logger fields). The response includes PromptTokens and CompletionTokens which are accumulated but not persisted.\n\n## Evidence\n- brain/planner.go:156-162 — `resp, err := p.llm.ChatWithTools(ctx, req)` in planning loop\n  - ctx has IssueID via logger.WithLogFields (set by orchestrator)\n  - resp.PromptTokens and resp.CompletionTokens accumulated in totalPromptTokens/totalCompletionTokens\n- brain/explore_agent.go:280-286 — `resp, err := e.llm.ChatWithTools(ctx, req)` in explore loop\n  - ctx has IssueID via logger.WithLogFields (inherited from planner)\n  - resp.PromptTokens tracked in contextWindowTokens, resp.CompletionTokens accumulated\n- brain/orchestrator.go:94-100 — Sets IssueID in context before calling planner:\n  ```go\n  ctx = logger.WithLogFields(ctx, logger.LogFields{\n      IssueID: &input.IssueID,\n  })\n  ```\n\n## Snippets\n```go\n// brain/planner.go:156-162 - ChatWithTools in planner\nresp, err := p.llm.ChatWithTools(ctx, llm.AgentRequest{\n    Messages:  messages,\n    Tools:     p.tools,\n    MaxTokens: 8000,\n})\n// Token accumulation:\ntotalPromptTokens += resp.PromptTokens\ntotalCompletionTokens += resp.CompletionTokens\n\n// brain/explore_agent.go:280-286 - ChatWithTools in explore\nresp, err := e.llm.ChatWithTools(ctx, llm.AgentRequest{\n    Messages:  messages,\n    Tools:     e.tools.AsLLMTools(),\n    MaxTokens: 4000,\n})\ncontextWindowTokens = resp.PromptTokens\ntotalCompletionTokens += resp.CompletionTokens\n```\n\n## Confidence\nhigh — Both call sites have IssueID in context and track tokens locally\n\n---\n\n**Confidence Assessment:** high — The call sites are clear, tokens are available but not persisted."
    },
    {
      "id": "llm-eval-store-call-sites",
      "intent": "Understanding where LLMEvalStore methods are called from in Go code",
      "keywords": ["InsertLLMEval", "LLMEvalStore", "Create", "call sites", "usage", "persist", "write", "insert", "Go code", "caller", "who calls"],
      "scope": ["store/llm_eval.go", "internal/", "service/", "brain/"],
      "report": "## Answer\nThe LLMEvalStore interface and InsertLLMEval SQLC method exist but are **NEVER CALLED** anywhere in the codebase. The infrastructure is ready but unused - no Go code currently persists LLM eval records.\n\n## Evidence\n- grep -r \"InsertLLMEval\" — Only found in:\n  - core/db/queries/llm_evals.sql (definition)\n  - core/db/sqlc/llm_evals.sql.go (generated code)\n  - store/llm_eval.go (wrapper method)\n  - NO call sites in brain/, service/, or anywhere else\n- grep -r \"LLMEvalStore\" — Only found in:\n  - store/interfaces.go (interface definition)\n  - store/stores.go (factory method)\n  - NO injection or usage in orchestrator, planner, or services\n- grep -r \"\\.Create(\" store/llm_eval.go — The Create method wraps InsertLLMEval but is never called\n\n## Snippets\n```bash\n# Search results - InsertLLMEval is defined but never called:\n$ grep -rn \"InsertLLMEval\" relay/\ncore/db/queries/llm_evals.sql:1:-- name: InsertLLMEval :one\ncore/db/sqlc/llm_evals.sql.go:15:func (q *Queries) InsertLLMEval(...)\nstore/llm_eval.go:23:    return q.InsertLLMEval(ctx, ...)\n\n# No call sites in business logic:\n$ grep -rn \"llmEvals\\|LLMEvalStore\" relay/internal/\n(no results - store is never injected or used)\n```\n\n## Confidence\nhigh — Exhaustive grep confirms zero call sites\n\n---\n\n**Confidence Assessment:** high — The store exists but is completely unused. Token tracking needs to wire up the planner/orchestrator to call LLMEvalStore.Create() after each LLM call or at end of planning."
    },
    {
      "id": "logger-context-fields",
      "intent": "Understanding how logger.WithLogFields works and how to extract IssueID from context",
      "keywords": ["WithLogFields", "GetLogFields", "LogFields", "IssueID", "context", "extract", "logger", "retrieve", "ctx"],
      "scope": ["common/logger/context.go", "common/logger/"],
      "report": "## Answer\nThe `logger` package provides `WithLogFields` to enrich context and `GetLogFields` to extract the `LogFields` struct which contains `IssueID *int64` and other business context.\n\n## Evidence\n- common/logger/context.go:15-25 — `LogFields` struct definition:\n  - `IssueID *int64` — Relay issue ID\n  - `EventLogID *int64` — Event log ID\n  - `IntegrationID *int64` — Integration ID\n  - `WorkspaceID *int64` — Workspace ID\n  - `Component string` — Component name\n- common/logger/context.go:28-35 — `WithLogFields(ctx, fields)` adds LogFields to context\n- common/logger/context.go:38-45 — `GetLogFields(ctx)` retrieves LogFields from context\n\n## Snippets\n```go\n// common/logger/context.go:15-25\ntype LogFields struct {\n    IssueID       *int64  // Relay issue ID\n    EventLogID    *int64  // Event log ID\n    MessageID     *string // Redis stream message ID\n    IntegrationID *int64  // Integration ID\n    WorkspaceID   *int64  // Workspace ID\n    EventType     *string // Event type\n    Component     string  // Component name\n}\n\n// common/logger/context.go:28-35\nfunc WithLogFields(ctx context.Context, fields LogFields) context.Context {\n    existing := GetLogFields(ctx)\n    merged := mergeFields(existing, fields)\n    return context.WithValue(ctx, logFieldsKey, merged)\n}\n\n// common/logger/context.go:38-45\nfunc GetLogFields(ctx context.Context) LogFields {\n    if fields, ok := ctx.Value(logFieldsKey).(LogFields); ok {\n        return fields\n    }\n    return LogFields{}\n}\n\n// Usage to get IssueID:\nfields := logger.GetLogFields(ctx)\nif fields.IssueID != nil {\n    issueID := *fields.IssueID\n}\n```\n\n## Confidence\nhigh — Direct code inspection of logger context utilities\n\n---\n\n**Confidence Assessment:** high — GetLogFields(ctx).IssueID is the canonical way to extract IssueID from context."
    },
    {
      "id": "planner-construction",
      "intent": "Understanding how Planner is constructed and what dependencies it receives",
      "keywords": ["NewPlanner", "planner.New", "brain.NewPlanner", "constructor", "dependencies", "injection", "Planner struct", "constructed"],
      "scope": ["brain/planner.go", "brain/orchestrator.go"],
      "report": "## Answer\nPlanner is constructed via `brain.NewPlanner(llm, explore, debugDir)` in the orchestrator. It receives an LLM client, an ExploreAgent, and a debug directory. It does NOT currently receive any store dependencies.\n\n## Evidence\n- brain/planner.go:79-83 — `Planner` struct fields:\n  - `llm llm.AgentClient` — LLM client for ChatWithTools\n  - `explore *ExploreAgent` — Sub-agent for code exploration\n  - `debugDir string` — Directory for debug logs\n- brain/planner.go:85-92 — `NewPlanner` constructor:\n  - Takes `llm llm.AgentClient`, `explore *ExploreAgent`, `debugDir string`\n  - Returns `*Planner`\n- brain/orchestrator.go:136 — Construction site in NewOrchestrator:\n  - `planner := NewPlanner(agentClient, explore, debugDir)`\n\n## Snippets\n```go\n// brain/planner.go:79-92\ntype Planner struct {\n    llm      llm.AgentClient\n    explore  *ExploreAgent\n    debugDir string\n}\n\nfunc NewPlanner(llm llm.AgentClient, explore *ExploreAgent, debugDir string) *Planner {\n    return &Planner{\n        llm:      llm,\n        explore:  explore,\n        debugDir: debugDir,\n    }\n}\n\n// brain/orchestrator.go:136 — Where Planner is created\nexplore := NewExploreAgent(agentClient, tools, cfg.ModulePath, debugDir)\nplanner := NewPlanner(agentClient, explore, debugDir)\n```\n\n## Confidence\nhigh — Planner construction is straightforward, no store injection yet\n\n---\n\n**Confidence Assessment:** high — To add token persistence, Planner would need LLMEvalStore injected, or orchestrator handles persistence after Plan() returns."
    }
  ]
}
